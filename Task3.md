##  Report for task 3
1. Замеры на файле `wiki.txt` с помощью `MRJob` показали следующие результаты:
  * `real 1m16.978s`
  * `user 1m15.634s`
  * `sys  0m1.340s`
2.  Замеры на файле `wiki.txt` с помощью `Hadoop`:
  * `real 0m54.460s`
  * `user 0m15.307s`
  * `sys  0m1.986s`
3. Замеры на файле `wiki_trunc.txt` (первые 50 строк из `wiki.txt`) с помощью `MRJob`:
  * `real 0m2.569s`
  * `user 0m2.499s`
  * `sys  0m0.070s`
4. Замеры на файле `wiki_trunc.txt` (первые 50 строк из `wiki.txt`) с помощью `Hadoop`:
  * `real 0m21.875s`
  * `user 0m15.206s`
  * `sys  0m1.628s`

###  Summary
На большом файле время выполнения `Hadoop` и `Python` с локальным движком `MRJob` довольно близко, однако нужно учитывать, 
что замеры проводились в рамках одной машины, тогда как `Hadoop` умеет в горизонтальное масштабирование --> при увеличении
числа нод скорость обработки увеличится.
На меньшем файле `Python` быстрее, так как `Hadoop` имеет дополнительные накладные расходы на организацию задач, 
которые при большом объеме данных не так критичны, как при малых.

**Вывод**: используйте для решения своей задачи нужный инструмент.
